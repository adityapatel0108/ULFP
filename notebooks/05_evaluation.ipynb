{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Evaluation\n",
        "\n",
        "This notebook evaluates models on standard face recognition benchmarks:\n",
        "- LFW, CFP-FP, AgeDB-30, CALFW, CPLFW\n",
        "- Unlearning-specific metrics\n",
        "\n",
        "## Objectives\n",
        "1. Evaluate face verification performance\n",
        "2. Compute unlearning metrics\n",
        "3. Compare original vs unlearned models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import pandas as pd\n",
        "\n",
        "from utils.model_loader import load_model_from_config\n",
        "from linearizer.linearizer import Linearizer\n",
        "from evaluation.benchmark import BenchmarkRunner\n",
        "from evaluation.verification import evaluate_lfw\n",
        "\n",
        "# Load configuration\n",
        "with open('../config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Evaluate Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load original model\n",
        "original_model = load_model_from_config(config)\n",
        "original_model = original_model.to(device)\n",
        "original_model.eval()\n",
        "\n",
        "# Evaluate on LFW\n",
        "lfw_path = config['data']['evaluation']['lfw']\n",
        "lfw_pairs_file = os.path.join(lfw_path, 'pairs.txt')\n",
        "\n",
        "try:\n",
        "    if os.path.exists(lfw_path):\n",
        "        print(\"Evaluating on LFW...\")\n",
        "        lfw_results = evaluate_lfw(original_model, lfw_path, lfw_pairs_file, device)\n",
        "        print(\"\\nLFW Results:\")\n",
        "        for key, value in lfw_results.items():\n",
        "            if isinstance(value, dict):\n",
        "                print(f\"{key}:\")\n",
        "                for k, v in value.items():\n",
        "                    print(f\"  {k}: {v:.4f}\")\n",
        "            else:\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"LFW dataset not found at {lfw_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating LFW: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Evaluate Unlearned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load linearized model (assuming already trained and unlearned)\n",
        "# In practice, you would load from checkpoint\n",
        "linearizer_config = config['linearizer']\n",
        "linearizer = Linearizer(\n",
        "    model=original_model,\n",
        "    embedding_size=config['model'].get('embedding_size', 512),\n",
        "    num_blocks=linearizer_config.get('num_blocks', 4)\n",
        ")\n",
        "linearizer = linearizer.to(device)\n",
        "linearizer.eval()\n",
        "\n",
        "# Evaluate unlearned model\n",
        "try:\n",
        "    if os.path.exists(lfw_path):\n",
        "        print(\"Evaluating unlearned model on LFW...\")\n",
        "        lfw_results_unlearned = evaluate_lfw(linearizer, lfw_path, lfw_pairs_file, device)\n",
        "        print(\"\\nLFW Results (Unlearned):\")\n",
        "        for key, value in lfw_results_unlearned.items():\n",
        "            if isinstance(value, dict):\n",
        "                print(f\"{key}:\")\n",
        "                for k, v in value.items():\n",
        "                    print(f\"  {k}: {v:.4f}\")\n",
        "            else:\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating unlearned model: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
