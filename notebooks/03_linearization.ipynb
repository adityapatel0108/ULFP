{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linearization Framework - Sandwich Architecture\n",
        "\n",
        "This notebook demonstrates the Linearizer framework using the **sandwich architecture**:\n",
        "- f(x) = g⁻¹ᵧ(Agₓ(x))\n",
        "- gₓ: Image → Latent (invertible network)\n",
        "- A: Linear operator in latent space\n",
        "- g⁻¹ᵧ: Latent → Embedding (invertible network)\n",
        "\n",
        "Based on: Berman et al. \"Who Said Neural Networks Aren't Linear?\" (2025)\n",
        "\n",
        "## Objectives\n",
        "1. Load InsightFace buffalo_l model\n",
        "2. Create Linearizer with sandwich architecture\n",
        "3. Train invertible networks (gₓ and g⁻¹ᵧ) and linear operator A\n",
        "4. Evaluate reconstruction quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.model_loader import load_model_from_config\n",
        "from linearizer.linearizer import Linearizer\n",
        "from data.dataloader import get_ms1mv2_dataloader\n",
        "\n",
        "# Load configuration\n",
        "with open('../config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Face Recognition Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the original face recognition model\n",
        "model = load_model_from_config(config)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Original model loaded\")\n",
        "embedding_size = config['model'].get('embedding_size', 512)\n",
        "print(f\"Embedding size: {embedding_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Linearizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Linearizer with sandwich architecture\n",
        "linearizer_config = config['linearizer']\n",
        "latent_dim = linearizer_config.get('latent_dim', 512)\n",
        "image_size = (112, 112)  # Standard face recognition image size\n",
        "\n",
        "linearizer = Linearizer(\n",
        "    model=model,\n",
        "    embedding_size=embedding_size,\n",
        "    latent_dim=latent_dim,\n",
        "    num_blocks=linearizer_config.get('num_blocks', 4),\n",
        "    hidden_dim=linearizer_config.get('hidden_dim', 1024),\n",
        "    num_layers=linearizer_config.get('num_layers', 3),\n",
        "    image_size=image_size\n",
        ")\n",
        "linearizer = linearizer.to(device)\n",
        "\n",
        "print(\"Linearizer created successfully with sandwich architecture!\")\n",
        "print(f\"  - gₓ: Image ({image_size}) → Latent ({latent_dim})\")\n",
        "print(f\"  - A: Linear operator ({latent_dim} × {latent_dim})\")\n",
        "print(f\"  - g⁻¹ᵧ: Latent ({latent_dim}) → Embedding ({embedding_size})\")\n",
        "print(f\"  - Invertible blocks per network: {linearizer_config.get('num_blocks', 4)}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in linearizer.parameters())\n",
        "trainable_params = sum(p.numel() for p in linearizer.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Linearizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "ms1mv2_path = config['data']['ms1mv2']['path']\n",
        "dataloader = get_ms1mv2_dataloader(\n",
        "    ms1mv2_path,\n",
        "    batch_size=linearizer_config.get('batch_size', 64),\n",
        "    num_workers=4,\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "# Train linearizer\n",
        "print(\"Training linearizer...\")\n",
        "linearizer.train_linearizer(\n",
        "    dataloader,\n",
        "    num_epochs=linearizer_config.get('num_epochs', 100),\n",
        "    lr=linearizer_config.get('learning_rate', 0.0001),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate Reconstruction Quality\n",
        "\n",
        "Test the linearized model's ability to reconstruct original embeddings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Sandwich Architecture Flow\n",
        "\n",
        "Let's visualize how the sandwich architecture processes an image step-by-step:\n",
        "- Input image → gₓ → Latent space → A (linear op) → g⁻¹ᵧ → Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test reconstruction quality\n",
        "linearizer.eval()\n",
        "test_dataloader = get_ms1mv2_dataloader(\n",
        "    ms1mv2_path,\n",
        "    batch_size=32,\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "reconstruction_errors = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        \n",
        "        # Original embeddings\n",
        "        original_emb = model.extract_features(images)\n",
        "        \n",
        "        # Linearized embeddings\n",
        "        linearized_emb = linearizer(images)\n",
        "        \n",
        "        # Compute reconstruction error\n",
        "        error = torch.nn.functional.mse_loss(original_emb, linearized_emb)\n",
        "        reconstruction_errors.append(error.item())\n",
        "\n",
        "avg_error = np.mean(reconstruction_errors)\n",
        "print(f\"Average reconstruction error: {avg_error:.6f}\")\n",
        "\n",
        "# Visualize some examples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "with torch.no_grad():\n",
        "    images, _ = next(iter(test_dataloader))\n",
        "    images = images[:5].to(device)\n",
        "    \n",
        "    original_emb = model.extract_features(images)\n",
        "    linearized_emb = linearizer(images)\n",
        "    \n",
        "    for i in range(5):\n",
        "        # Show image\n",
        "        axes[0, i].imshow(images[i].cpu().permute(1, 2, 0) * 0.5 + 0.5)\n",
        "        axes[0, i].set_title(\"Input Image\")\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # Show embedding difference\n",
        "        diff = (original_emb[i] - linearized_emb[i]).cpu().numpy()\n",
        "        axes[1, i].bar(range(min(20, len(diff))), diff[:20])\n",
        "        axes[1, i].set_title(f\"Embedding Diff\\nMSE: {torch.nn.functional.mse_loss(original_emb[i], linearized_emb[i]):.6f}\")\n",
        "        axes[1, i].set_ylim(-0.1, 0.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the sandwich architecture flow\n",
        "linearizer.eval()\n",
        "with torch.no_grad():\n",
        "    # Get a sample batch\n",
        "    images, _ = next(iter(test_dataloader))\n",
        "    images = images[:1].to(device)  # Single image for visualization\n",
        "    \n",
        "    # Step-by-step processing\n",
        "    print(\"Sandwich Architecture Flow:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Step 1: gₓ(x) - Image to Latent\n",
        "    z = linearizer.g_x(images, reverse=False)\n",
        "    print(f\"1. Input image shape: {images.shape}\")\n",
        "    print(f\"2. After gₓ (latent): {z.shape}\")\n",
        "    print(f\"   Latent vector norm: {torch.norm(z).item():.4f}\")\n",
        "    \n",
        "    # Step 2: A(z) - Linear operator\n",
        "    z_transformed = linearizer.linear_op(z)\n",
        "    print(f\"3. After A (linear op): {z_transformed.shape}\")\n",
        "    print(f\"   Transformed latent norm: {torch.norm(z_transformed).item():.4f}\")\n",
        "    \n",
        "    # Step 3: g⁻¹ᵧ(z) - Latent to Embedding\n",
        "    embedding = linearizer.g_y_inv(z_transformed, reverse=False)\n",
        "    print(f\"4. After g⁻¹ᵧ (embedding): {embedding.shape}\")\n",
        "    print(f\"   Embedding norm: {torch.norm(embedding).item():.4f}\")\n",
        "    \n",
        "    # Compare with original\n",
        "    original_emb = model.extract_features(images)\n",
        "    print(f\"\\n5. Original embedding: {original_emb.shape}\")\n",
        "    print(f\"   Original embedding norm: {torch.norm(original_emb).item():.4f}\")\n",
        "    \n",
        "    # Compute similarity\n",
        "    similarity = torch.nn.functional.cosine_similarity(embedding, original_emb, dim=1)\n",
        "    print(f\"\\nCosine similarity: {similarity.item():.6f}\")\n",
        "    print(f\"MSE: {torch.nn.functional.mse_loss(embedding, original_emb).item():.6f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
